{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP3_GROUP2_master2TAL_2122.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP3: using pre-trained embeddings and tuning a NN\n",
        "\n",
        "In this practical session, we will tune a neural network model, thus modifying the values of the hyper-parameters. We will also explore changes in the architecture. \n",
        "\n",
        "The dataset comes from IMDb (Internet Movie Database). The task is binary genre classification from movie description. Here we use the training data, that we will split into a training set, a validation set and a test set.\n",
        "\n",
        "Our base model will be a FFNN or a RNN with **continuous representations intialized with pre-trained word embeddings**. The embeddings used are GloVe with 50 dimensions, built using Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). **All tokens are in lowercase**.\n",
        "\n",
        "Upload the files: \n",
        "* train_....txt\n",
        "* glove.6B.50d.txt.gz (it can take several minutes).\n",
        "\n",
        "You are required to:\n",
        "- read carefully all the instructions\n",
        "- add code when asked\n",
        "- answer questions and add comments in a form of a short report about the experiments \n",
        "\n",
        "There is a total of 15 questions with the id Q1, .., Q15. \n",
        "\n",
        "You can send either directly a notebook containing the code + report, or the code in a notebook and a separate file with the report. Put your files on Modle (Controle continu - Groupe 1 ou 2). The filename must contain your name.\n",
        "\n",
        "Due date: \n",
        "- Group 2: 11/03 "
      ],
      "metadata": {
        "id": "0ySDOfB5zBgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random, io\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "torch.manual_seed(0) # For reproducibility: https://pytorch.org/docs/stable/notes/randomness.html"
      ],
      "metadata": {
        "id": "z0NPaYGKBZaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "U84s4nyFCnYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1: read the data\n"
      ],
      "metadata": {
        "id": "pff1DLTGANv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Dataset\n",
        "\n",
        "The code below allows to read and load the dataset for genre classification."
      ],
      "metadata": {
        "id": "YAlsccOaFntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"train_drama-comedy_group2.txt\"\n",
        "\n",
        "label_mapping = {'drama':0, 'comedy':1}\n",
        "labels = set()\n",
        "fin = io.open(train_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "data_iter = []\n",
        "\n",
        "for line in fin:\n",
        "  id, title, label, text = line.split(':::')\n",
        "  label = label.strip()\n",
        "  labels.add(label)\n",
        "  # lower case, because GloVe contains lower cased words\n",
        "  data_iter.append( tuple([label, text.lower().strip() ]) )\n",
        "\n",
        "print(\"Labels:\", labels)\n",
        "print(\"Total number of examples:\",len(data_iter))\n",
        "\n",
        "# List of examples (label, text)\n",
        "train = data_iter[:20709]\n",
        "dev = data_iter[20709:23709] \n",
        "test =  data_iter[23709:] \n",
        "\n",
        "print(\"Train:\", len(train), \"Valid:\", len(dev), \"Test:\", len(test))"
      ],
      "metadata": {
        "id": "45I4f1iHCOpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now, we need to tokenize our data, and build the corresponding vocabulary (on the train set).\n",
        "\n",
        "#### Q1: **Print the size of the vocabulary.**"
      ],
      "metadata": {
        "id": "pS2jdf2Vt1Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# splits the string sentence by space.\n",
        "tokenizer = get_tokenizer( None ) \n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: label_mapping[x]\n",
        "\n",
        "## Print the size of the vocabulary\n",
        "# .."
      ],
      "metadata": {
        "id": "ZBctRAcZsQSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Loading the word embeddings\n",
        "\n",
        "We will use the GloVe pre-trained word emeddings, with 50 dimensions, as contained in the file glove.6B.50d.txt.gz."
      ],
      "metadata": {
        "id": "V_CkLYiSGy5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2: **Write a function that loads the vectors** \n",
        "This function allows to build a dictionary mapping a word to its vector, as defined in the GloVe file. \n"
      ],
      "metadata": {
        "id": "RX2DkAqws1gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io, gzip\n",
        "\n",
        "## Write a function that loads the vectors\n",
        "# and build a dictionnary mapping a token to its vector\n",
        "# ...\n",
        "#def load_vectors(fname):\n",
        "\n",
        "\n",
        "embed_file='glove.6B.50d.txt.gz'\n",
        "#vectors = load_vectors( embed_file )"
      ],
      "metadata": {
        "id": "dqwtkQvl3ZMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3: **Print the following information about the embeddings:** \n",
        "\n",
        "* print the vector for the token 'the'\n",
        "* print the vocabulary of the GloVe embeddings\n",
        "* print the size/dimension of the embeddings\n",
        "* compare the vocabulary of the embeddings with the vocabulary built on training set: \n",
        "  * How many words in your data do not appear in the embeddings vocabulary?\n",
        "  * Do you think it could be an issue? why?\n",
        "  * Why do we have all these unknown words?"
      ],
      "metadata": {
        "id": "qSEK2RY9IPDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about the embeddings\n",
        "# ..."
      ],
      "metadata": {
        "id": "yd2EEjECv4vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4: **Propose a solution to reduce the number of unknown words.**\n",
        "\n"
      ],
      "metadata": {
        "id": "tPkhU8q8I_r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B82LSQh43kWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Building batches (code given)\n",
        "\n",
        "The function below can be used to build batches of examples based on offsets, as used in e.g. EmbeddingBag."
      ],
      "metadata": {
        "id": "3Q4i7BC9J80V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label.to(device), text_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "eJ1F3u0ODibY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2: Feed Forward Neural Network with pretrained embeddings\n",
        "\n",
        "Now we need to define our learning model. \n",
        "\n",
        "First, we're going to build a matrix containing the embedding vectors of the words in our vocabulary (i.e. the words present in the training set). Then, we will use this matrix to initialize our embedding layer."
      ],
      "metadata": {
        "id": "UDlM7OZq56HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Build the weights matrix\n",
        "\n",
        "We want to build the *weights matrix* that will be used to initialize our embedding layer. \n",
        "In this matrix, we associate each word in our (training) data to the vector retrieved from the GloVe file."
      ],
      "metadata": {
        "id": "GTA0vXeevSuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5: **Build a matrix associating each word to its vector:** \n",
        "  * For each word in the dataset’s vocabulary, we check if it is in GloVe’s vocabulary:\n",
        "    * if yes: load its pre-trained word vector. \n",
        "    * else: initialize a random vector.\n",
        "    \n",
        "At the end, print the matrix size. "
      ],
      "metadata": {
        "id": "u9zhBKH1K4sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 50\n",
        "matrix_len = len(vocab)\n",
        "weights_matrix = np.zeros((matrix_len, emb_dim))\n",
        "\n",
        "# Build the weights matrix\n",
        "# ...\n",
        "\n",
        "# At the end, cast the weight matrix to float32\n",
        "weights_matrix = weights_matrix.to(torch.float32)"
      ],
      "metadata": {
        "id": "4XXFTaRxvRNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Model definition\n",
        "\n",
        "Now we can define our model with an embedding layer that takes pretrained embeddings.\n",
        "\n",
        "The code is very similar to what we had previously, except that we need to initialize the embedding layer using the weight matrix."
      ],
      "metadata": {
        "id": "VcLWQgu877rQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6: **Define the embedding layer using the weight matrix.**\n",
        "\n",
        "You need to build embedding bags using pretrained embeddings. Let the embeddings to be trainable (i.e. not freezed). Look at the documentation for the *nn.EmbeddingBag*: https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html"
      ],
      "metadata": {
        "id": "0keXrjT5MnyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7: **Add the other parts of the code:** \n",
        "  * *__init__()* function: add the first linear layer, the activation function, and the linear output layer (note that the 'input_dim' has been replaced with 'embed_dim': the input of our model is embeddings bags).\n",
        "  * *forward()* function: get the input through the embedding layer, the hidden layer (i.e. linear function + activation function) and the output layer."
      ],
      "metadata": {
        "id": "j0E17flXM2Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedforwardNeuralNetModel2(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, output_dim, weights_matrix):\n",
        "        super(FeedforwardNeuralNetModel2, self).__init__()\n",
        "        ## Create the embedding layer from the weight matrix\n",
        "        # ...\n",
        "\n",
        "        # Linear function\n",
        "        # ...\n",
        "\n",
        "        # Non-linearity\n",
        "        # ...\n",
        "\n",
        "        # Linear function (readout)\n",
        "        # ... \n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        # Embedding layer\n",
        "        # ...\n",
        "\n",
        "        # Linear function  \n",
        "        # ...\n",
        "\n",
        "        # Non-linearity  \n",
        "        # ...\n",
        "\n",
        "        # Linear function (readout) \n",
        "        # ...\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "fXOPuCv_vZrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Train and evaluate (code given)\n",
        "\n",
        "The functions below can be used to train and evaluate your model (they are designed to work when dealing with batches and EmbeddingBag, i.e. they use the 'offsets' associated with the batches in the input)."
      ],
      "metadata": {
        "id": "Xpwmr42v0CBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_woffset( model, train_loader, optimizer, num_epochs=5 ):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, total_acc, total_count = 0, 0, 0\n",
        "        for label, input, offsets in train_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            # Step1. Clearing the accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Step 2. Forward pass to get output/logits\n",
        "            outputs = model( input, offsets )\n",
        "            # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            # - Calculate Loss: softmax --> cross entropy loss\n",
        "            loss = criterion(outputs, label)\n",
        "            # - Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "            # - Updating parameters\n",
        "            optimizer.step()\n",
        "            # Accumulating the loss over time\n",
        "            train_loss += loss.item()\n",
        "            total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "        # Compute accuracy on train set at each epoch\n",
        "        print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/total_count, total_acc/total_count))\n",
        "        total_acc, total_count = 0, 0\n",
        "        train_loss = 0\n",
        "\n",
        "def evaluate_woffset( model, dev_loader ):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "    with torch.no_grad():\n",
        "        for label, input, offsets in dev_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            probs = model(input, offsets)\n",
        "            predictions.extend( torch.argmax(probs, dim=1).cpu().numpy() ) # <-----\n",
        "            gold.extend([int(l) for l in label])\n",
        "    print(classification_report(gold, predictions))\n",
        "    return gold, predictions"
      ],
      "metadata": {
        "id": "US_0JmN5phqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q8: **Run an experiment**\n",
        "\n",
        "Now we are ready to run a first experiment using our model.\n",
        "\n",
        "▶▶ **Run a first experiment with:**\n",
        "* Learning rate: 0.01\n",
        "* Batch size: 64\n",
        "* Hidden dimension: 16\n",
        "* Epochs: 5\n",
        "* Optimizer: SGD\n",
        "\n",
        "Note that:\n",
        "* Loss is still the Cross Entropy Loss\n",
        "* Evaluation is done on the development set"
      ],
      "metadata": {
        "id": "rUUw-q6E0Vo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a first experiment with the setting described above\n",
        "\n",
        "# Hyperparameters\n",
        "# ...\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 0.01  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "hidden_dim = 16 # size of the hidden layer\n",
        "\n",
        "output_dim = 2\n",
        "emb_dim = 50\n",
        "\n",
        "# Load the data\n",
        "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "dev_loader = DataLoader(dev, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Initialize the model\n",
        "# ...\n",
        "\n",
        "# Define the loss and optimization method to be used\n",
        "# ...\n",
        "\n",
        "# Train the model\n",
        "# ...\n",
        "\n",
        "# Evaluate on dev\n",
        "# ..."
      ],
      "metadata": {
        "id": "ClMTL35_CVc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 3: Tune the model \n",
        "\n",
        "The model comes with a variety of hyper-parameters. To find the best model, we need to test different values for these free parameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1HmIthzRumir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q9: **Test different values of the hyper-parameters:**\n",
        "\n",
        "We want to see what could be the best performance of the defined model, without modifying the input. Use the development set to tune your model, that is compare the performance when modifying the following elements. At each step, you can keep the best value found at the previous step (or test a few combinations).\n",
        "\n",
        "* a) Optimizer: try at least Adam, Adagrad and RMSProp (in addition to SGD)\n",
        "* b) Batch size: test a few values, e.g. 1 (= no batch), 256, 2048 (in addition to 64, already tested above)\n",
        "* c) Max number of epochs: test with 20, 50, 100 epochs. Do you think 100 epochs are required? Can you decide on a 'reasonable' (trade-off between speed and performance) number of epochs?\n",
        "* d) Learning rate: test a few values e.g. 0.00001, 1 (in addition to 0.01). What do you observe?\n",
        "* e) Size of the hidden layer: test a few values, e.g. 4, 128, 1024 (in addition to 16)\n",
        "\n",
        " \n",
        "Propose a final best set of parameters, based on your experiments, and present final results on the test set.\n",
        "\n",
        "Try to propose a small report using tables and/or plots that shows how performance change with respect to the different hyper-parameters. "
      ],
      "metadata": {
        "id": "j_cPuUTS2ZBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oJ6982o36cxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q10: **Modify the architecture of the FFNN:**\n",
        "\n",
        "Now modify your model definition to test with:\n",
        "* a) A different activation function: test at least tanh and ReLu (in addition to sigmoid)\n",
        "* b) One additional hidden layer\n",
        "\n",
        "Don't try to change again the hyper-parameters, give the results with the best set previously obtained and any value for the second hidden dimension."
      ],
      "metadata": {
        "id": "S6Lh7oe4aSK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KyNTMzNbX2Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 4: RNNs\n",
        "\n",
        "Now we want to test a RNN architecture to perform our classification task. In the following exercises, we **will not use the pre-trained word embeddings** but randomly initialized continuous vectors with 300  dimensions.\n",
        "\n",
        "Note that here, the embedding layer transforms our words into continuous vectors that are the inputs of our RNN. The RNN builds the document representation and is thus a replacement of the 'embedding bag'. "
      ],
      "metadata": {
        "id": "DxutwLmHHUdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Using Batches (code given)\n",
        "\n",
        "When using RNNs, we can't use the offset trick to build batches: the problem is that all the documents in a batch need to have the same length, because the size of the input defines the size of the network (i.e. each xi is associated with a state si). \n",
        "\n",
        "The solution is called **padding**: we add zeros at the end of the sequences that are shorter than the max length. \n",
        "\n",
        "The easiest solution to do so is to pad the sequences using *torch.nn.utils.rnn.pad_sequence* as done below within the *collate_batch_pad* function. This function returns a tensor of padded sequences, that can be directly used as input of our model.\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html"
      ],
      "metadata": {
        "id": "Yl5OW_hwHUdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch_pad(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "    label = torch.tensor(label_list, dtype=torch.int64)\n",
        "    # Instead of concatenating, we use padding\n",
        "    text_list = pad_sequence(text_list, padding_value=0) # <-------\n",
        "    return label.to(device), text_list.to(device)"
      ],
      "metadata": {
        "id": "iElEypvDHUd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 LSTM\n",
        "\n",
        "First, we'll test an LSTM. In PyTorch, defining an LSTM is done via the addition of an LSTM layer in the architecture.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C17YKygAHUd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q11: **Add an embedding layer.**\n",
        "\n",
        "Note that here we're not using *embedding bags*, but just an embedding layer that maps our word to vectors. The representation of the document is then obtained via the RNN. \n",
        "\n",
        "See: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n"
      ],
      "metadata": {
        "id": "mIXZsk5cHUd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q12: **Add the LSTM layer.** \n",
        "\n",
        "An LSTM layer will transform our input into a vector representation with the size hidden_dim.\n",
        "Add the LSTM layer and the output layer (we don't put additional hidden layer here).\n",
        "\n",
        "See: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KNgg4WXVHUd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forward (code given)\n",
        "\n",
        "Note that in the forward pass, we need to reshape the data using:\n",
        "```\n",
        "x = x.view(len(x), 1, -1)\n",
        "```\n",
        "\n",
        "We need to reshape our input data before passing it to the LSTM layer, because it takes a 3D tensor with (Sequence lenght, Batch size, Input size). This is done with the 'view' method, the pytorch 'reshape' function for tensors.\n",
        "\n",
        "Read the code carefuly to be sure you understand how we build the representation using the LSTM, that is using its last hidden state.\n",
        "\n",
        "```\n",
        "out, (ht, ct) = self.lstm( x )\n",
        "y = self.fc2(ht[-1])\n",
        "```\n"
      ],
      "metadata": {
        "id": "TT7GUVnPIirA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Embedding layer\n",
        "        # ...\n",
        "\n",
        "        # LSTM layer\n",
        "        # ...\n",
        "\n",
        "        # Linear fct\n",
        "        # ...\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "\n",
        "    def forward(self, text):\n",
        "        embeds = self.embedding(text)\n",
        "        x = embeds.view(len(text), self.batch_size, -1)\n",
        "        out, (ht, ct) = self.lstm( x )\n",
        "        y = self.fc2(ht[-1])\n",
        "        return y\n",
        "\n",
        "\n",
        "# These fct are modified to ignore offsets\n",
        "def train_lstm( model, train_loader, optimizer, num_epochs=5 ):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, total_acc, total_count = 0, 0, 0\n",
        "        for label, input in train_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            # Step1. Clearing the accumulated gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Step 2. Forward pass to get output/logits\n",
        "            outputs = model( input )\n",
        "            # Step 3. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            # - Calculate Loss: softmax --> cross entropy loss\n",
        "            loss = criterion(outputs, label)\n",
        "            # - Getting gradients w.r.t. parameters\n",
        "            loss.backward()\n",
        "            # - Updating parameters\n",
        "            optimizer.step()\n",
        "            # Accumulating the loss over time\n",
        "            train_loss += loss.item()\n",
        "            total_acc += (outputs.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "        # Compute accuracy on train set at each epoch\n",
        "        print('Epoch: {}. Loss: {}. ACC {} '.format(epoch, train_loss/total_count, total_acc/total_count))\n",
        "        total_acc, total_count = 0, 0\n",
        "        train_loss = 0\n",
        "\n",
        "def evaluate( model, dev_loader ):\n",
        "    predictions = []\n",
        "    gold = []\n",
        "    with torch.no_grad():\n",
        "        for label, input in dev_loader:\n",
        "            input = input.to(device)\n",
        "            label = label.to(device)\n",
        "            probs = model(input)\n",
        "            predictions.extend( torch.argmax(probs, dim=1).cpu().numpy() ) # <-----\n",
        "            gold.extend([int(l) for l in label])\n",
        "    print(classification_report(gold, predictions))\n",
        "    return gold, predictions"
      ],
      "metadata": {
        "id": "sT7LgfMyHUd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q13: **Run an experiment using the following hyper-parameters**\n",
        "\n",
        "We can now test our LSTM on our dataset. \n",
        "\n",
        "* epochs = 5 \n",
        "* learning rate =  0.001\n",
        "* size of the hidden layer = 32\n",
        "* batch size = 128 \n",
        "* embbeding dimension = 300\n",
        "* optimizer: Adam"
      ],
      "metadata": {
        "id": "zXKk2BXSHUd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 0.001  # learning rate\n",
        "hidden_dim = 32 # size of the hidden layer\n",
        "batch_size = 128 \n",
        "emb_dim = 300\n",
        "\n",
        "output_dim = 2\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Load data\n",
        "train_loader = DataLoader(train, shuffle=False, batch_size=batch_size, \n",
        "                          collate_fn=collate_batch_pad, drop_last=True)\n",
        "dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_size, \n",
        "                        collate_fn=collate_batch_pad, drop_last=True)\n",
        "\n",
        "# Initialize the model\n",
        "# ...\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# ...\n",
        "\n",
        "# Evaluate on dev\n",
        "# ..."
      ],
      "metadata": {
        "id": "xgyHT2vnHUd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 GRU and bi-GRU\n",
        "\n",
        "We now want to try another RNN architecture called GRU: https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n"
      ],
      "metadata": {
        "id": "cCKeZ3rSHUd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Q14: **Modify your code to implement a GRU instead of an LSTM, and run an experiment (same setting).** \n",
        "\n",
        "Here you also need to modify the forward function."
      ],
      "metadata": {
        "id": "NJQVqOzKHUd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        # ...\n",
        "\n",
        "        # GRU layer\n",
        "        # self.gru = \n",
        "\n",
        "        # Linear fct\n",
        "        # ...  \n",
        "\n",
        "    def forward(self, text):\n",
        "        embeds = self.embedding(text)\n",
        "        x = embeds.view(len(text), self.batch_size, -1)\n",
        "        # Compute y\n",
        "        # ...\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "J7p5E5pNHUd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 0.001  # learning rate\n",
        "hidden_dim = 32 # size of the hidden layer\n",
        "batch_size = 128 \n",
        "emb_dim = 300\n",
        "\n",
        "output_dim = 2\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Load data\n",
        "train_loader = DataLoader(train, shuffle=False, batch_size=batch_size, \n",
        "                          collate_fn=collate_batch_pad, drop_last=True)\n",
        "dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_size, \n",
        "                        collate_fn=collate_batch_pad, drop_last=True)\n",
        "\n",
        "# Initialize the model\n",
        "# ...\n",
        "\n",
        "# Train the model\n",
        "# ...\n",
        "\n",
        "# Evaluate on dev\n",
        "# ..."
      ],
      "metadata": {
        "id": "JYg6-8hQHUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q15: **Modify your code to implement a bi-GRU, i.e. a bi-directional GRU and run an experiment (same setting).** \n",
        "\n",
        "Which architecture gave the best results? Looking at the performance on training and development set, what could you say?\n",
        "\n",
        "Hint: be careful, what is the size of the output of a bi-RNN?"
      ],
      "metadata": {
        "id": "77rKlCD5HUd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiGRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        # Embedding layer\n",
        "        # ...\n",
        "\n",
        "        # biGRU layer\n",
        "        # ...\n",
        "\n",
        "        # linear layer\n",
        "        # ...\n",
        "\n",
        "    def forward(self, text):\n",
        "        embeds = self.embedding(text)\n",
        "        x = embeds.view(len(text), self.batch_size, -1)\n",
        "        # Compute y\n",
        "        # ...\n",
        "        \n",
        "        return y"
      ],
      "metadata": {
        "id": "57lP5HqtHUd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 0.001  # learning rate\n",
        "hidden_dim = 32 # size of the hidden layer\n",
        "batch_size = 128\n",
        "emb_dim = 300\n",
        "\n",
        "output_dim = 2\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Load data\n",
        "train_loader = DataLoader(train, shuffle=False, batch_size=batch_size, \n",
        "                          collate_fn=collate_batch_pad, drop_last=True)\n",
        "dev_loader = DataLoader(dev, shuffle=False, batch_size=batch_size, \n",
        "                        collate_fn=collate_batch_pad, drop_last=True)\n",
        "\n",
        "# Initialize the model\n",
        "# ...\n",
        "\n",
        "# Train the model\n",
        "# ...\n",
        "\n",
        "# Evaluate on dev\n",
        "# ..."
      ],
      "metadata": {
        "id": "s5n9Szd4HUd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}